{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 19 Exercise: File Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama Speech:\n",
      "Number of Lines:  0\n",
      "Number of words  0\n",
      "\n",
      "\n",
      "Michelle Obama Speech:\n",
      "Number of Lines:  0\n",
      "Number of words  0\n",
      "\n",
      "\n",
      "Donald Speech:\n",
      "Number of Lines:  48\n",
      "Number of words  7233\n",
      "\n",
      "\n",
      "Melina Trump Speech:\n",
      "Number of Lines:  0\n",
      "Number of words  0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a function which count number of lines and number of words in a text. \n",
    "# All the files are in the data the folder: \n",
    "# a) Read obama_speech.txt file and count number of lines and words \n",
    "\n",
    "with open('./obama_speech.txt', 'r') as f:\n",
    "    num_words = 0\n",
    "    num_lines = 0\n",
    "    for line in f:\n",
    "        num_lines += 1\n",
    "        num_words += len(line)\n",
    "\n",
    "    print(\"Obama Speech:\")\n",
    "    print(\"Number of Lines: \", num_lines)\n",
    "    print(\"Number of words \", num_words)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# b) Read michelle_obama_speech.txt file and count number of lines and words \n",
    "\n",
    "with open('./michelle_obama_speech.txt', 'r') as f:\n",
    "    num_words = 0\n",
    "    num_lines = 0\n",
    "    for line in f:\n",
    "        num_lines += 1\n",
    "        num_words += len(line)\n",
    "\n",
    "    print(\"Michelle Obama Speech:\")\n",
    "    print(\"Number of Lines: \", num_lines)\n",
    "    print(\"Number of words \", num_words)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# c) Read donald_speech.txt file and count number of lines and words\n",
    "\n",
    "with open('./donald_speech.txt', 'r') as f:\n",
    "    num_words = 0\n",
    "    num_lines = 0\n",
    "    for line in f:\n",
    "        num_lines += 1\n",
    "        num_words += len(line)\n",
    "\n",
    "    print(\"Donald Speech:\")\n",
    "    print(\"Number of Lines: \", num_lines)\n",
    "    print(\"Number of words \", num_words)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# d) Read melina_trump_speech.txt file and count number of lines and words\n",
    "\n",
    "with open('./melina_trump_speech.txt', 'r') as f:\n",
    "    num_words = 0\n",
    "    num_lines = 0\n",
    "    for line in f:\n",
    "        num_lines += 1\n",
    "        num_words += len(line)\n",
    "\n",
    "    print(\"Melina Trump Speech:\")\n",
    "    print(\"Number of Lines: \", num_lines)\n",
    "    print(\"Number of words \", num_words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('English', 91), ('French', 45), ('Arabic', 25), ('Spanish', 24), ('Portuguese', 9), ('Russian', 9), ('Dutch', 8), ('German', 7), ('Chinese', 5), ('Italian', 4)]\n"
     ]
    }
   ],
   "source": [
    "# Read the countries_data.json data file in data directory, \n",
    "# create a function that finds the ten most spoken languages\n",
    "import json\n",
    "\n",
    "def most_spoken_lang(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        all_languages = [language for country in data for language in country.get('languages', [])]\n",
    "        language_count = {language: all_languages.count(language) for language in set(all_languages)}\n",
    "\n",
    "        most_spoken_languages = sorted(language_count.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        return most_spoken_languages\n",
    "\n",
    "file_path = './countries_data.json'\n",
    "print(most_spoken_lang(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('China', 1377422166), ('India', 1295210000), ('United States of America', 323947000), ('Indonesia', 258705000), ('Brazil', 206135893), ('Pakistan', 194125062), ('Nigeria', 186988000), ('Bangladesh', 161006790), ('Russian Federation', 146599183), ('Japan', 126960000)]\n"
     ]
    }
   ],
   "source": [
    "# Read the countries_data.json data file in data directory, \n",
    "# create a function that creates a list of the ten most populated countries\n",
    "\n",
    "import json\n",
    "\n",
    "def most_populous(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        country_population = {country['name']: country['population'] for country in data}\n",
    "        most_populated_countries = sorted(country_population.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        return most_populated_countries\n",
    "\n",
    "file_path = './countries_data.json'\n",
    "print(most_populous(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './email_exchange_big.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract all incoming email addresses as a list from the email_exchange_big.txt file.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./email_exchange_big.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     all_emails \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './email_exchange_big.txt'"
     ]
    }
   ],
   "source": [
    "# Extract all incoming email addresses as a list from the email_exchange_big.txt file.\n",
    "import re\n",
    "\n",
    "with open('./email_exchange_big.txt', 'r') as f:\n",
    "    all_emails = []\n",
    "    for line in f:\n",
    "        pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "        emails = re.findall(pattern, line)\n",
    "        if len(emails) == 0:\n",
    "           continue\n",
    "        else:\n",
    "            all_emails.append(emails)\n",
    "    print(all_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sample', 2), ('a', 2), ('show', 1), ('This', 1), ('just', 1)]\n",
      "[('sample', 2), ('a', 2), ('show', 1), ('This', 1), ('just', 1), ('statement', 1), ('text', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Find the most common words in the English language. \n",
    "# Call the name of your function find_most_common_words, \n",
    "# it will take two parameters - a string or a file and a positive integer, indicating the number of words. \n",
    "# Your function will return an array of tuples in descending order.\n",
    "import os \n",
    "def find_most_common_words(file_path, num):\n",
    "    words = []\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                words.extend([word for word in line.split(\" \")])\n",
    "    else:\n",
    "        words.extend([word for word in file_path.split(\" \")])\n",
    "\n",
    "    freq = {i:words.count(i) for i in set(words)}\n",
    "    sorted_freq = sorted(freq.items(), key=lambda x:x[1], reverse=True)[:num]\n",
    "    return sorted_freq\n",
    "\n",
    "print(find_most_common_words(\"This is a sample statement just to show a sample in the text\", 5))\n",
    "print(find_most_common_words(\"This is a sample statement just to show a sample in the text\", 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama speech ten most common words:\n",
      "[]\n",
      "\n",
      "\n",
      "Michelle speech ten most common words:\n",
      "[]\n",
      "\n",
      "\n",
      "Trump speech ten most common words:\n",
      "[('the', 61), ('and', 53), ('will', 40), ('of', 38), ('to', 32), ('our', 30), ('we', 26), ('is', 20), ('We', 15), ('America', 14)]\n",
      "\n",
      "\n",
      "Melina speech ten most common words:\n",
      "[]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the function, find_most_frequent_words to find: \n",
    "# a) The ten most frequent words used in Obama's speech \n",
    "print(\"Obama speech ten most common words:\")\n",
    "print(find_most_common_words(\"./obama_speech.txt\", 10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# b) The ten most frequent words used in Michelle's speech \n",
    "print(\"Michelle speech ten most common words:\")\n",
    "print(find_most_common_words(\"./michelle_obama_speech.txt\", 10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# c) The ten most frequent words used in Trump's speech \n",
    "print(\"Trump speech ten most common words:\")\n",
    "print(find_most_common_words(\"./donald_speech.txt\", 10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# d) The ten most frequent words used in Melina's speech\n",
    "print(\"Melina speech ten most common words:\")\n",
    "print(find_most_common_words(\"./melina_trump_speech.txt\", 10))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two transcript are similar\n",
      "The two transcript are similar\n"
     ]
    }
   ],
   "source": [
    "# Write a python application that checks similarity between two texts. \n",
    "# It takes a file or a string as a parameter and it will evaluate the similarity of the two texts. \n",
    "# For instance check the similarity between the transcripts of Michelle's and Melina's speech. \n",
    "# You may need a couple of functions, function to clean the text(clean_text), \n",
    "# function to remove support words(remove_support_words) and finally to check the similarity(check_text_similarity). \n",
    "# List of stop words are in the data directory\n",
    "import os\n",
    "import re\n",
    "\n",
    "def similarity_checker(file1_path, file2_path):\n",
    "    file1_content = []\n",
    "    file2_content = []\n",
    "\n",
    "    if os.path.isfile(file1_path) and os.path.isfile(file2_path):\n",
    "        with open(file1_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                file1_content.extend(line)\n",
    "\n",
    "        with open(file2_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                file2_content.extend(line)\n",
    "    \n",
    "    else:\n",
    "        file1_content.extend(file1_path)\n",
    "        file2_content.extend(file2_path)\n",
    "\n",
    "    # Function to remove clean text\n",
    "    def clean_text(text1, text2):\n",
    "        txt1 = [re.sub('[.,;\\']', '', word).lower() for line in text1 for word in line]\n",
    "        txt2 = [re.sub('[.,;\\']', '', word).lower() for line in text2 for word in line]\n",
    "\n",
    "        return txt1, txt2\n",
    "    \n",
    "    tran1, tran2 = clean_text(file1_content, file2_content)\n",
    "\n",
    "    # Function to remove stopwords \n",
    "    def remove_stopwords(text1, text2):\n",
    "        stop_words = ['i','me','my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up','down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "        txt1 = [word for line in text1 for word in line.split(\" \") if word not in stop_words]\n",
    "        txt2 = [word for line in text2 for word in line.split(\" \") if word not in stop_words]\n",
    "\n",
    "        return txt1, txt2\n",
    "    \n",
    "    # Check for similarity\n",
    "    transcript1, transcript2 = remove_stopwords(tran1, tran2)\n",
    "\n",
    "    if transcript1 == transcript2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "print(\"The two transcript are similar\") if similarity_checker('./michelle_obama_speech.txt', './melina_trump_speech.txt') else print(\"The two transcript are not similar\")\n",
    "print(\"The two transcript are similar\") if similarity_checker('./melina_trump_speech.txt', './melina_trump_speech.txt') else print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './romeo_and_juliet.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         sorted_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(freq\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m sorted_freq\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe 10 most repeated word are: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmost_repeated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./romeo_and_juliet.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mmost_repeated\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmost_repeated\u001b[39m(file_path):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m         words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './romeo_and_juliet.txt'"
     ]
    }
   ],
   "source": [
    "# Find the 10 most repeated words in the romeo_and_juliet.txt\n",
    "\n",
    "def most_repeated(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "            words.extend([word for word in line.split(\" \")])\n",
    "\n",
    "        freq = {i:words.count(i) for i in set(words)}\n",
    "        sorted_freq = sorted(freq.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "        return sorted_freq\n",
    "    \n",
    "print(\"The 10 most repeated word are: \", most_repeated('./romeo_and_juliet.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the hacker news csv file and find out: \n",
    "# a) Count the number of lines containing python or Python \n",
    "# b) Count the number lines containing JavaScript, javascript or Javascript \n",
    "# c) Count the number lines containing Java and not JavaScript\n",
    "import csv\n",
    "\n",
    "def  count_words(file_path):\n",
    "    python_count = 0\n",
    "    javascript_count = 0\n",
    "    java_count = 0\n",
    "\n",
    "    with open(file_path, 'r') as csv:\n",
    "        data = csv.reader(csv, delimiter=',')\n",
    "        for row in data:\n",
    "            if bool(re.match(r'P|python', row[0])):\n",
    "                python_count += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        for row in data:\n",
    "            if bool(re.match(r'J|javaS|script', row[0])):\n",
    "                javascript_count += 1\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        for row in data:\n",
    "            if bool(re.match(r'Java', row[0])):\n",
    "                java_count += 1\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        return python_count, javascript_count, java_count\n",
    "\n",
    "\n",
    "# print(\"Number of counts: Python: {}, Javascript {}, Java {}\", count_words(\"./path\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
